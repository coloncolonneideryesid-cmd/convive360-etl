#!/usr/bin/env python3
"""
Pipeline ETL para Convive360 - Alcald√≠a Local de San Crist√≥bal
Versi√≥n CORREGIDA v4:
  - 0 registros perdidos: drop_duplicates solo por clave de negocio real
  - 0 celdas en blanco artificiales: columnas exclusivas de cada formulario
    se rellenan con 'N/A' o valor por defecto tras el concat
  - Todos los CSV con sep=';' y encoding='utf-8-sig'
  - ID_Actividad √∫nico garantizado (incluye √≠ndice de fila)
  - Genera todas las columnas/tablas que necesita el modelo Power BI
"""

import os
import sys
import pandas as pd
from google.oauth2 import service_account
from googleapiclient.discovery import build
import logging
from datetime import datetime
from typing import Dict
import hashlib

# ========================================
# CONFIGURACI√ìN
# ========================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pipeline_log.txt'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

SPREADSHEET_ID   = os.getenv('SPREADSHEET_ID', '15DIXQnQfS9_gbYC4h3j5inIgEVEUJC79ceCc4uYv_ts')
SHEET_NAME_1     = os.getenv('SHEET_NAME_1',   'Respuestas de formulario 1')
SHEET_NAME_2     = os.getenv('SHEET_NAME_2',   'Respuestas de formulario 2')
CREDENTIALS_FILE = 'credentials.json'

CSV_SEP = ';'
CSV_ENC = 'utf-8-sig'

# Clave de negocio para detectar duplicados REALES
# (mismo formulario enviado dos veces accidentalmente)
CLAVE_DUPLICADO = ['Marca_Temporal', 'Email_Responsable', 'Nombre_Actividad']

# Columnas que SOLO existen en Formulario 1 ‚Üí en F2 quedar√°n vac√≠as tras concat
SOLO_FORMULARIO_1 = [
    'Enmarca_En',
    'Barrios_UPZ32',
    'Barrios_UPZ33',
    'Barrios_UPZ34',
    'Barrios_UPZ51',
    'Barrios_UPZ50',
    'Linea_Seguridad',
    'Linea_Justicia',
    'Linea_Convivencia',  # en F1 tiene nombre distinto pero se mapea igual
]

# Columnas que SOLO existen en Formulario 2 ‚Üí en F1 quedar√°n vac√≠as tras concat
SOLO_FORMULARIO_2 = [
    'Enfoque_Estrategico',
]

# ========================================
# MAPEO DE COLUMNAS
# ========================================

COLUMN_MAPPING = {
    'Marca temporal':                                 'Marca_Temporal',
    'Direcci√≥n de correo electr√≥nico':                'Email_Responsable',
    '2. Nombre de la actividad':                      'Nombre_Actividad',
    'Nombre de la Actividad':                         'Nombre_Actividad',
    '3. Descripci√≥n de la actividad':                 'Descripcion_Actividad',
    'Descripci√≥n de la Actividad':                    'Descripcion_Actividad',
    '4. Responsables de la actividad':                'Responsable_Principal',
    'Responsables de la actividad':                   'Responsable_Principal',
    '5. Con quien va articular':                      'Articulacion',
    'Con qui√©n va a articular':                       'Articulacion',
    '6. Responsable de la actividad':                 'Responsable_Actividad',
    '4. Responsable de la actividad*':                'Responsable_Actividad',
    '5. Enfoque de la actividad':                     'Enfoque_Actividad',
    'Enfoque de la actividad*':                       'Enfoque_Actividad',
    'Enfoque Estrat√©gico':                            'Enfoque_Estrategico',
    '6. Estrategia a impactar':                       'Estrategia_Impactar',
    'Estrategia de Impacto':                          'Estrategia_Impactar',
    '1. Esta actividad se enmarca en:':               'Enmarca_En',
    '6.1. L√≠neas Estrat√©gicas de Seguridad':          'Linea_Seguridad',
    'L√≠neas Estrat√©gicas de Seguridad':               'Linea_Seguridad',
    '6.2. L√≠neas Estrat√©gicas de Convivencia':        'Linea_Convivencia',
    'L√≠neas Estrat√©gicas de Convivencia':             'Linea_Convivencia',
    '6.3. L√≠neas Estrat√©gicas de Justicia':           'Linea_Justicia',
    'L√≠neas Estrat√©gicas de Justicia':                'Linea_Justicia',
    '8. UPZ a la Que Pertenece la Actividad':         'UPZ',
    'UPZ a la Que Pertenece la Actividad':            'UPZ',
    'BARRIOS DE LA UPZ 32 - San Blas':                'Barrios_UPZ32',
    'BARRIOS DE LA UPZ 33 - Sosiego':                 'Barrios_UPZ33',
    'BARRIOS DE LA UPZ 34 - 20 de Julio':             'Barrios_UPZ34',
    'BARRIOS DE LA UPZ 51 - Los Libertadores':        'Barrios_UPZ51',
    'BARRIOS DE LA UPZ 50 - La Gloria':               'Barrios_UPZ50',
    '9. Zona a la que Pertenece la Actividad':        'Zona',
    'Zona a la que Pertenece la Actividad':           'Zona',
    '7. Direcci√≥n donde se realiza la actividad':     'Direccion_Actividad',
    'Direcci√≥n donde se realiza la actividad':        'Direccion_Actividad',
    '10. Fecha de la actividad':                      'Fecha_Actividad',
    'Fecha de Actividad':                             'Fecha_Actividad',
    '11. Hora de inicio':                             'Hora_Inicio',
    'Hora de Inicio de Actividad':                    'Hora_Inicio',
    '12. ¬øDeseas recibir un correo de confirmaci√≥n?': 'Recibir_Correo',
    '¬øDeseas recibir un correo de confirmaci√≥n?':     'Recibir_Correo',
    'Estado':                                         'Estado',
    'ID del evento':                                  'ID_Evento',
    'Quien rechaz√≥':                                  'Quien_Rechazo',
    'Fecha de cancelaci√≥n':                           'Fecha_Cancelacion',
}

# ========================================
# AUTENTICACI√ìN
# ========================================

def autenticar_google_sheets() -> object:
    if not os.path.exists(CREDENTIALS_FILE):
        raise FileNotFoundError(f"No se encontr√≥: {CREDENTIALS_FILE}")
    credentials = service_account.Credentials.from_service_account_file(
        CREDENTIALS_FILE,
        scopes=[
            'https://www.googleapis.com/auth/spreadsheets.readonly',
            'https://www.googleapis.com/auth/drive.readonly',
        ]
    )
    service = build('sheets', 'v4', credentials=credentials)
    logger.info("‚úÖ Autenticaci√≥n exitosa")
    return service

# ========================================
# EXTRACCI√ìN
# ========================================

def extraer_datos(service: object, sheet_name: str) -> pd.DataFrame:
    logger.info(f"üì• Extrayendo: {sheet_name}")
    result = service.spreadsheets().values().get(
        spreadsheetId=SPREADSHEET_ID,
        range=f"{sheet_name}!A:AB"
    ).execute()
    values = result.get('values', [])
    if not values:
        logger.warning(f"‚ö†Ô∏è Sin datos en {sheet_name}")
        return pd.DataFrame()
    # Normalizar filas con menos columnas que el header (evita errores de longitud)
    header = values[0]
    rows = [row + [''] * (len(header) - len(row)) for row in values[1:]]
    df = pd.DataFrame(rows, columns=header)
    logger.info(f"‚úÖ {len(df)} registros de {sheet_name}")
    return df

# ========================================
# NORMALIZACI√ìN POR HOJA
# ========================================

def normalizar_columnas(df: pd.DataFrame) -> pd.DataFrame:
    """
    Renombra y consolida columnas en un DataFrame INDIVIDUAL antes del concat.
    Evita el error 'cannot assemble with duplicate keys'.
    Tambi√©n elimina espacios extras en nombres de columnas (ej: √∫ltima col de F2).
    """
    # Limpiar espacios en nombres de columnas
    df.columns = [c.strip() for c in df.columns]

    nombres_usados: Dict[str, list] = {}
    for col in df.columns:
        nombre_final = COLUMN_MAPPING.get(col, col)
        nombres_usados.setdefault(nombre_final, []).append(col)

    nuevas_columnas = {}
    for nombre_final, cols_originales in nombres_usados.items():
        if len(cols_originales) == 1:
            nuevas_columnas[cols_originales[0]] = nombre_final
        else:
            col_destino = cols_originales[0]
            for col_extra in cols_originales[1:]:
                df[col_destino] = (
                    df[col_destino].replace('', pd.NA)
                    .fillna(df[col_extra].replace('', pd.NA))
                )
                df = df.drop(columns=[col_extra])
            nuevas_columnas[col_destino] = nombre_final

    df = df.rename(columns=nuevas_columnas)
    df = df.loc[:, ~df.columns.duplicated(keep='first')]
    return df

# ========================================
# LIMPIEZA Y ENRIQUECIMIENTO
# ========================================

def limpiar_datos(df: pd.DataFrame, hoja_origen: str) -> pd.DataFrame:
    """
    Limpia una hoja individual ANTES del concat.
    NO hace drop_duplicates aqu√≠ (se hace despu√©s del concat con clave real).
    """
    logger.info(f"üßπ Limpiando: {hoja_origen} ({len(df)} registros)")

    # Eliminar filas donde TODAS las columnas est√©n vac√≠as
    df = df.replace('', pd.NA)
    df = df.dropna(how='all')
    df = df.replace(pd.NA, '')  # Volver a string vac√≠o para manejo consistente

    # Etiqueta de origen
    df['Hoja_Origen'] = hoja_origen

    # Fechas
    for col in ['Marca_Temporal', 'Fecha_Actividad', 'Fecha_Cancelacion']:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], dayfirst=True, errors='coerce')

    logger.info(f"  ‚úì {hoja_origen}: {len(df)} registros limpios")
    return df


def rellenar_columnas_cruzadas(df: pd.DataFrame) -> pd.DataFrame:
    """
    FIX CELDAS EN BLANCO:
    Tras el concat, las columnas exclusivas de cada formulario quedan NaN
    en las filas del otro formulario. Las rellenamos con 'N/A' para que
    Power BI no muestre celdas vac√≠as.
    
    Formulario 1 tiene: Enmarca_En, Barrios_*, Linea_Seguridad, Linea_Justicia
    Formulario 2 tiene: Enfoque_Estrategico, solo Linea_Convivencia
    Ambos pueden no tener: Articulacion, Estado, ID_Evento, etc.
    """
    # Columnas que deben existir siempre con valor por defecto
    defaults = {
        # Columnas opcionales de formulario 1
        'Enmarca_En':        'No especificado',
        'Barrios_UPZ32':     'N/A',
        'Barrios_UPZ33':     'N/A',
        'Barrios_UPZ34':     'N/A',
        'Barrios_UPZ51':     'N/A',
        'Barrios_UPZ50':     'N/A',
        'Linea_Seguridad':   'No aplica',
        'Linea_Justicia':    'No aplica',
        'Linea_Convivencia': 'No aplica',
        # Columnas opcionales de formulario 2
        'Enfoque_Estrategico': 'No especificado',
        # Columnas opcionales generales
        'Articulacion':       'N/A',
        'Estado':             'Pendiente',
        'ID_Evento':          '',
        'Quien_Rechazo':      '',
        'Fecha_Cancelacion':  '',
        'Recibir_Correo':     'No',
        'Enfoque_Actividad':  'No especificado',
        'Responsable_Actividad': 'No especificado',
    }

    for col, default_val in defaults.items():
        if col in df.columns:
            # Reemplazar NaN y string vac√≠o con el valor por defecto
            df[col] = df[col].replace('', pd.NA).fillna(default_val)
        else:
            # Si la columna no existe en absoluto, crearla con el default
            df[col] = default_val

    logger.info("  ‚úì Columnas cruzadas rellenadas con valores por defecto")
    return df


def enriquecer_datos(df: pd.DataFrame) -> pd.DataFrame:
    """
    Genera las columnas derivadas que usa el modelo Power BI:
    UPZ_Enriquecida, Zona_Enriquecida, Estrategia, Barrio_Extraido.
    """
    # Columnas enriquecidas requeridas por Power BI
    df['UPZ_Enriquecida']  = df['UPZ'].astype(str).str.strip()  if 'UPZ'  in df.columns else 'Sin UPZ'
    df['Zona_Enriquecida'] = df['Zona'].astype(str).str.strip()  if 'Zona' in df.columns else 'Sin Zona'
    df['Estrategia'] = (
        df['Estrategia_Impactar'].astype(str).str.strip()
        if 'Estrategia_Impactar' in df.columns else 'Sin estrategia'
    )

    # Barrio consolidado desde columnas por UPZ
    df['Barrio_Extraido'] = 'Sin barrio'
    upz_barrio_map = {
        '32': 'Barrios_UPZ32',
        '33': 'Barrios_UPZ33',
        '34': 'Barrios_UPZ34',
        '51': 'Barrios_UPZ51',
        '50': 'Barrios_UPZ50',
    }
    if 'UPZ' in df.columns:
        for idx, row in df.iterrows():
            upz = str(row.get('UPZ', ''))
            for num, col_barrio in upz_barrio_map.items():
                if num in upz and col_barrio in df.columns:
                    valor = str(row.get(col_barrio, '')).strip()
                    if valor and valor not in ('N/A', 'nan', ''):
                        df.at[idx, 'Barrio_Extraido'] = valor
                    break

    logger.info("  ‚úì Columnas enriquecidas generadas")
    return df

# ========================================
# ID √öNICO
# ========================================

def generar_id_actividad(row, idx: int) -> str:
    """
    Hash MD5 con √≠ndice de fila incluido ‚Üí unicidad absoluta garantizada.
    Soluciona: 'ID_Actividad contiene un valor duplicado'
    """
    campos = (
        f"{idx}"                                    # √≠ndice garantiza unicidad
        f"|{row.get('Marca_Temporal', '')}"
        f"|{row.get('Nombre_Actividad', '')}"
        f"|{row.get('Fecha_Actividad', '')}"
        f"|{row.get('Hora_Inicio', '')}"
        f"|{row.get('Direccion_Actividad', '')}"
        f"|{row.get('Email_Responsable', '')}"
        f"|{row.get('Hoja_Origen', '')}"
    )
    return hashlib.md5(campos.encode()).hexdigest()[:16].upper()

# ========================================
# DIMENSIONES
# ========================================

def crear_dimensiones(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
    logger.info("üî® Creando dimensiones")
    dimensiones = {}

    def dim_simple(col: str, id_col: str, nombre: str):
        if col not in df.columns:
            logger.warning(f"  ‚ö†Ô∏è Columna '{col}' no encontrada para {nombre}")
            return
        d = df[[col]].copy()
        d[col] = d[col].astype(str).str.strip()
        d = d[d[col].notna() & (d[col] != '') & (d[col] != 'nan')]
        d = d.drop_duplicates().reset_index(drop=True)
        d.insert(0, id_col, range(1, len(d) + 1))
        dimensiones[nombre] = d
        logger.info(f"  ‚úì {nombre}: {len(d)} valores √∫nicos")

    dim_simple('UPZ_Enriquecida',  'upz_id',        'dim_upz')
    dim_simple('Zona_Enriquecida', 'zona_id',        'dim_zonas')
    dim_simple('Estrategia',       'estrategia_id',  'dim_estrategias')
    dim_simple('Enfoque_Actividad','enfoque_id',     'dim_enfoques')
    dim_simple('Estado',           'estado_id',      'dim_estados')

    # dim_barrios
    if 'Barrio_Extraido' in df.columns and 'UPZ_Enriquecida' in df.columns:
        d = df[['Barrio_Extraido', 'UPZ_Enriquecida']].copy()
        d['Barrio_Extraido']  = d['Barrio_Extraido'].astype(str).str.strip()
        d['UPZ_Enriquecida']  = d['UPZ_Enriquecida'].astype(str).str.strip()
        d = d[d['Barrio_Extraido'].notna() & (d['Barrio_Extraido'] != '') & (d['Barrio_Extraido'] != 'Sin barrio')]
        d = d.drop_duplicates().reset_index(drop=True)
        d.insert(0, 'barrio_id', range(1, len(d) + 1))
        dimensiones['dim_barrios'] = d
        logger.info(f"  ‚úì dim_barrios: {len(d)} valores √∫nicos")

    # Dim_Lineas_Estrategicas ‚Üí requerida por USERELATIONSHIP en DAX
    lineas = []
    for col_linea, tipo in [
        ('Linea_Seguridad',   'Seguridad'),
        ('Linea_Convivencia', 'Convivencia'),
        ('Linea_Justicia',    'Justicia'),
    ]:
        if col_linea in df.columns:
            vals = df[['ID_Actividad', col_linea]].copy()
            vals[col_linea] = vals[col_linea].astype(str).str.strip()
            vals = vals[
                vals[col_linea].notna() &
                (vals[col_linea] != '') &
                (vals[col_linea] != 'nan') &
                (vals[col_linea] != 'No aplica')
            ]
            vals = vals.rename(columns={col_linea: 'Linea_Estrategica'})
            vals['Tipo_Linea'] = tipo
            lineas.append(vals)

    if lineas:
        dim_lineas = pd.concat(lineas, ignore_index=True)
        dim_lineas.insert(0, 'linea_id', range(1, len(dim_lineas) + 1))
        dimensiones['Dim_Lineas_Estrategicas'] = dim_lineas
        logger.info(f"  ‚úì Dim_Lineas_Estrategicas: {len(dim_lineas)} registros")

    logger.info(f"‚úÖ {len(dimensiones)} dimensiones creadas")
    return dimensiones

# ========================================
# TABLA DE HECHOS
# ========================================

def crear_tabla_hechos(df: pd.DataFrame, dimensiones: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    logger.info("üìä Creando tabla de hechos")
    fact = df.copy()

    joins = [
        ('dim_upz',         'UPZ_Enriquecida',  'UPZ_Enriquecida'),
        ('dim_zonas',       'Zona_Enriquecida',  'Zona_Enriquecida'),
        ('dim_estrategias', 'Estrategia',         'Estrategia'),
        ('dim_enfoques',    'Enfoque_Actividad',  'Enfoque_Actividad'),
        ('dim_estados',     'Estado',             'Estado'),
    ]

    for nombre_dim, col_fact, col_dim in joins:
        if nombre_dim in dimensiones and col_fact in fact.columns:
            fact = fact.merge(
                dimensiones[nombre_dim],
                left_on=col_fact,
                right_on=col_dim,
                how='left'
            )

    if 'dim_barrios' in dimensiones and 'Barrio_Extraido' in fact.columns:
        fact = fact.merge(
            dimensiones['dim_barrios'],
            on=['Barrio_Extraido', 'UPZ_Enriquecida'],
            how='left'
        )

    # Limpiar columnas _x / _y generadas por los merges
    cols_limpias = [c for c in fact.columns if not c.endswith('_x') and not c.endswith('_y')]
    fact = fact[cols_limpias]

    logger.info(f"‚úÖ Tabla de hechos: {len(fact)} registros, {len(fact.columns)} columnas")
    return fact

# ========================================
# CARGA
# ========================================

def guardar_csv(df: pd.DataFrame, path: str) -> None:
    """Todos los CSV con sep=';' y utf-8-sig para Power BI."""
    df.to_csv(path, index=False, encoding=CSV_ENC, sep=CSV_SEP)
    logger.info(f"  ‚úì {path}  ({len(df)} registros)")


def guardar_archivos(fact: pd.DataFrame, dimensiones: Dict[str, pd.DataFrame]) -> None:
    logger.info("üíæ Guardando archivos")
    os.makedirs('dimensiones', exist_ok=True)

    guardar_csv(fact, 'fact_actividades.csv')
    guardar_csv(fact, 'fact_actividades_enriquecido.csv')
    guardar_csv(fact, 'dimensiones/fact_actividades_enriquecido.csv')

    for nombre, df_dim in dimensiones.items():
        guardar_csv(df_dim, f'dimensiones/{nombre}.csv')

    logger.info("‚úÖ Todos los archivos guardados")

# ========================================
# MAIN
# ========================================

def main():
    inicio = datetime.now()
    try:
        logger.info("=" * 60)
        logger.info("üöÄ INICIANDO PIPELINE ETL CONVIVE360 v4")
        logger.info("=" * 60)

        # 1. Autenticar
        service = autenticar_google_sheets()

        # 2. Extraer
        df1 = extraer_datos(service, SHEET_NAME_1)
        df2 = extraer_datos(service, SHEET_NAME_2)
        logger.info(f"  F1: {len(df1)} | F2: {len(df2)} | Esperado: {len(df1)+len(df2)}")

        # 3. Normalizar columnas POR SEPARADO (evita duplicate keys en concat)
        logger.info("üîÄ Normalizando columnas por hoja")
        df1 = normalizar_columnas(df1)
        df2 = normalizar_columnas(df2)

        # 4. Limpiar cada hoja individualmente
        df1 = limpiar_datos(df1, hoja_origen='Formulario_1')
        df2 = limpiar_datos(df2, hoja_origen='Formulario_2')

        # 5. Combinar
        logger.info("üîó Combinando hojas")
        df = pd.concat([df1, df2], ignore_index=True)
        df = df.loc[:, ~df.columns.duplicated(keep='first')]
        logger.info(f"  ‚úì Tras concat: {len(df)} registros")

        # 6. FIX CELDAS EN BLANCO: rellenar columnas exclusivas de cada formulario
        df = rellenar_columnas_cruzadas(df)

        # 7. Eliminar duplicados REALES (mismo formulario enviado dos veces)
        #    Solo por clave de negocio, NO por todas las columnas
        registros_antes = len(df)
        cols_dedup = [c for c in CLAVE_DUPLICADO if c in df.columns]
        if cols_dedup:
            df = df.drop_duplicates(subset=cols_dedup, keep='first')
            eliminados = registros_antes - len(df)
            if eliminados:
                logger.info(f"  ‚úì {eliminados} duplicado(s) real(es) eliminado(s)")
            else:
                logger.info(f"  ‚úì Sin duplicados reales ‚Äî todos los registros conservados")
        logger.info(f"  ‚úì Registros finales: {len(df)}")

        # 8. Generar columnas enriquecidas
        df = enriquecer_datos(df)

        # 9. ID √∫nico garantizado
        logger.info("üîë Generando ID_Actividad")
        df = df.reset_index(drop=True)  # √çndice limpio para el hash
        df['ID_Actividad'] = [
            generar_id_actividad(row, idx)
            for idx, row in df.iterrows()
        ]
        duplicados = df['ID_Actividad'].duplicated().sum()
        if duplicados:
            logger.warning(f"  ‚ö†Ô∏è {duplicados} IDs duplicados ‚Äî revisar datos fuente")
        else:
            logger.info(f"  ‚úì {len(df)} IDs √∫nicos generados")

        # 10. Dimensiones
        dimensiones = crear_dimensiones(df)

        # 11. Tabla de hechos
        fact = crear_tabla_hechos(df, dimensiones)

        # 12. Guardar
        guardar_archivos(fact, dimensiones)

        # Resumen
        duracion = (datetime.now() - inicio).total_seconds()
        logger.info("=" * 60)
        logger.info("üéâ PIPELINE COMPLETADO EXITOSAMENTE")
        logger.info(f"‚è±Ô∏è  Duraci√≥n     : {duracion:.2f} s")
        logger.info(f"üìä Registros    : {len(fact)}")
        logger.info(f"üìÅ Dimensiones  : {len(dimensiones)}")
        logger.info(f"üìã Columnas fact: {len(fact.columns)}")
        logger.info("=" * 60)
        return 0

    except Exception as e:
        import traceback
        logger.error("=" * 60)
        logger.error("üí• ERROR EN EL PIPELINE")
        logger.error(str(e))
        logger.error(traceback.format_exc())
        logger.error("=" * 60)
        return 1


if __name__ == "__main__":
    sys.exit(main())
